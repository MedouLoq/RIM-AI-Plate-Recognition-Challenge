{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXgTGsscqPWH"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Create a list of URLs for each page to scrape\n",
        "pages = [requests.get('https://www.voursa.com/Index.cfm?PN='+str(j)+'&gct=1&sct=11&gv=13') for j in range(2, 27)]\n",
        "\n",
        "urls = []\n",
        "\n",
        "# Loop through each page to extract image URLs\n",
        "for page in pages:\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    # Find all 'img' tags in the page\n",
        "    img_tags = soup.find_all('img')\n",
        "    # Extract the 'src' attribute from each 'img' tag\n",
        "    img_urls = [img['src'] for img in img_tags if 'src' in img.attrs]\n",
        "    # Add the extracted URLs to the 'urls' list\n",
        "    urls = urls + img_urls\n",
        "\n",
        "# Modify the URLs to replace 'smph' with 'bgph' and 'psp' with 'pbp' in the image path\n",
        "j = 1\n",
        "list=[]\n",
        "for url in urls:\n",
        "    url = url.replace(\"smph\", \"bgph\")\n",
        "    url = url.replace(\"psp\", \"pbp\")\n",
        "    if url.endswith('.jpg') and url.startswith('/produitsedr'):\n",
        "      list.append(url)\n",
        "    if j == 200:\n",
        "      break\n",
        "    j += 1\n",
        "\n",
        "# Open a file to save the links\n",
        "links = open('/content/links.txt', 'w')\n",
        "\n",
        "# Write the modified URLs to the file, only those ending with '.jpg' and starting with '/produitsedr'\n",
        "for url in list:\n",
        "  links.write('https://www.voursa.com' + url + '\\n')\n",
        "\n",
        "# Close the file after writing all the URLs\n",
        "links.close()\n",
        "\n",
        "# Reopen the file to read the links\n",
        "links = open('/content/links.txt', 'r')\n",
        "\n",
        "i = 1\n",
        "\n",
        "!mkdir imgs\n",
        "# Download each image from the URLs in the file\n",
        "for url in links.readlines():\n",
        "    # Open a file to save the image\n",
        "    with open('/content/imgs/web1_' + str(i) + '.jpg', 'wb') as handle:\n",
        "        response = requests.get(url.strip(), stream=True)\n",
        "\n",
        "        if not response.ok:\n",
        "            print(response)\n",
        "\n",
        "        # Write the image content to the file in chunks\n",
        "        for block in response.iter_content(1024):\n",
        "            if not block:\n",
        "                break\n",
        "            handle.write(block)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "# Close the links file\n",
        "links.close()\n",
        "\n",
        "# Zip the downloaded images\n",
        "!zip -r /content/imgs.zip /content/imgs/\n",
        "\n",
        "# Download the zip file containing the images\n",
        "from google.colab import files\n",
        "files.download(\"/content/imgs.zip\")\n"
      ]
    }
  ]
}